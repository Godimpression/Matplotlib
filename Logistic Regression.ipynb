{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages / libraries\n",
    "import os #provides functions for interacting with the operating system\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, explained_variance_score, confusion_matrix, accuracy_score, classification_report, log_loss\n",
    "from math import sqrt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# To install sklearn type \"pip install numpy scipy scikit-learn\" to the anaconda terminal\n",
    "\n",
    "# To change scientific numbers to float\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "\n",
    "# Increases the size of sns plots\n",
    "sns.set(rc={'figure.figsize':(12,10)})\n",
    "\n",
    "import sys\n",
    "# !conda list Check the packages installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at C:\\Users\\USER\\anaconda3:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_ipyw_jlab_nb_ext_conf    0.1.0            py39haa95532_0  \n",
      "alabaster                 0.7.12             pyhd3eb1b0_0  \n",
      "anaconda                  2021.11                  py39_0  \n",
      "anaconda-client           1.9.0            py39haa95532_0  \n",
      "anaconda-navigator        2.1.1                    py39_0  \n",
      "anaconda-project          0.10.1             pyhd3eb1b0_0  \n",
      "anyio                     2.2.0            py39haa95532_2  \n",
      "appdirs                   1.4.4              pyhd3eb1b0_0  \n",
      "argh                      0.26.2           py39haa95532_0  \n",
      "argon2-cffi               20.1.0           py39h2bbff1b_1  \n",
      "arrow                     0.13.1           py39haa95532_0  \n",
      "asn1crypto                1.4.0                      py_0  \n",
      "astroid                   2.6.6            py39haa95532_0  \n",
      "astropy                   4.3.1            py39hc7d831d_0  \n",
      "async_generator           1.10               pyhd3eb1b0_0  \n",
      "atomicwrites              1.4.0                      py_0  \n",
      "attrs                     21.2.0             pyhd3eb1b0_0  \n",
      "autopep8                  1.5.7              pyhd3eb1b0_0  \n",
      "babel                     2.9.1              pyhd3eb1b0_0  \n",
      "backcall                  0.2.0              pyhd3eb1b0_0  \n",
      "backports                 1.0                pyhd3eb1b0_2  \n",
      "backports.functools_lru_cache 1.6.4              pyhd3eb1b0_0  \n",
      "backports.shutil_get_terminal_size 1.0.0              pyhd3eb1b0_3  \n",
      "backports.tempfile        1.0                pyhd3eb1b0_1  \n",
      "backports.weakref         1.0.post1                  py_1  \n",
      "bcrypt                    3.2.0            py39h196d8e1_0  \n",
      "beautifulsoup4            4.10.0             pyh06a4308_0  \n",
      "binaryornot               0.4.4              pyhd3eb1b0_1  \n",
      "bitarray                  2.3.0            py39h2bbff1b_1  \n",
      "bkcharts                  0.2              py39haa95532_0  \n",
      "black                     19.10b0                    py_0  \n",
      "blas                      1.0                         mkl  \n",
      "bleach                    4.0.0              pyhd3eb1b0_0  \n",
      "blosc                     1.21.0               h19a0ad4_0  \n",
      "bokeh                     2.4.1            py39haa95532_0  \n",
      "boto                      2.49.0           py39haa95532_0  \n",
      "bottleneck                1.3.2            py39h7cc1a96_1  \n",
      "brotli                    1.0.9                ha925a31_2  \n",
      "brotlipy                  0.7.0           py39h2bbff1b_1003  \n",
      "bzip2                     1.0.8                he774522_0  \n",
      "ca-certificates           2021.10.26           haa95532_2  \n",
      "cached-property           1.5.2                      py_0  \n",
      "certifi                   2021.10.8        py39haa95532_0  \n",
      "cffi                      1.14.6           py39h2bbff1b_0  \n",
      "cfitsio                   3.470                he774522_6  \n",
      "chardet                   4.0.0           py39haa95532_1003  \n",
      "charls                    2.2.0                h6c2663c_0  \n",
      "charset-normalizer        2.0.4              pyhd3eb1b0_0  \n",
      "click                     8.0.3              pyhd3eb1b0_0  \n",
      "cloudpickle               2.0.0              pyhd3eb1b0_0  \n",
      "clyent                    1.2.2            py39haa95532_1  \n",
      "colorama                  0.4.4              pyhd3eb1b0_0  \n",
      "comtypes                  1.1.10          py39haa95532_1002  \n",
      "conda                     4.10.3           py39haa95532_0  \n",
      "conda-build               3.21.6           py39haa95532_0  \n",
      "conda-content-trust       0.1.1              pyhd3eb1b0_0  \n",
      "conda-env                 2.6.0                         1  \n",
      "conda-pack                0.6.0              pyhd3eb1b0_0  \n",
      "conda-package-handling    1.7.3            py39h8cc25b3_1  \n",
      "conda-repo-cli            1.0.4              pyhd3eb1b0_0  \n",
      "conda-token               0.3.0              pyhd3eb1b0_0  \n",
      "conda-verify              3.4.2                      py_1  \n",
      "console_shortcut          0.1.1                         4  \n",
      "contextlib2               0.6.0.post1        pyhd3eb1b0_0  \n",
      "cookiecutter              1.7.2              pyhd3eb1b0_0  \n",
      "cryptography              3.4.8            py39h71e12ea_0  \n",
      "curl                      7.78.0               h86230a5_0  \n",
      "cycler                    0.10.0           py39haa95532_0  \n",
      "cython                    0.29.24          py39h604cdb4_0  \n",
      "cytoolz                   0.11.0           py39h2bbff1b_0  \n",
      "daal4py                   2021.3.0         py39h757b272_0  \n",
      "dal                       2021.3.0           haa95532_564  \n",
      "dask                      2021.10.0          pyhd3eb1b0_0  \n",
      "dask-core                 2021.10.0          pyhd3eb1b0_0  \n",
      "dataclasses               0.8                pyh6d0b6a4_7  \n",
      "debugpy                   1.4.1            py39hd77b12b_0  \n",
      "decorator                 5.1.0              pyhd3eb1b0_0  \n",
      "defusedxml                0.7.1              pyhd3eb1b0_0  \n",
      "diff-match-patch          20200713           pyhd3eb1b0_0  \n",
      "distributed               2021.10.0        py39haa95532_0  \n",
      "docutils                  0.17.1           py39haa95532_1  \n",
      "entrypoints               0.3              py39haa95532_0  \n",
      "et_xmlfile                1.1.0            py39haa95532_0  \n",
      "fastcache                 1.1.0            py39h196d8e1_0  \n",
      "filelock                  3.3.1              pyhd3eb1b0_1  \n",
      "flake8                    3.9.2              pyhd3eb1b0_0  \n",
      "flask                     1.1.2              pyhd3eb1b0_0  \n",
      "fonttools                 4.25.0             pyhd3eb1b0_0  \n",
      "freetype                  2.10.4               hd328e21_0  \n",
      "fsspec                    2021.10.1          pyhd3eb1b0_0  \n",
      "future                    0.18.2           py39haa95532_1  \n",
      "get_terminal_size         1.0.0                h38e98db_0  \n",
      "gevent                    21.8.0           py39h2bbff1b_1  \n",
      "giflib                    5.2.1                h62dcd97_0  \n",
      "glob2                     0.7                pyhd3eb1b0_0  \n",
      "greenlet                  1.1.1            py39hd77b12b_0  \n",
      "h5py                      3.2.1            py39h3de5c98_0  \n",
      "hdf5                      1.10.6               h7ebc959_0  \n",
      "heapdict                  1.0.1              pyhd3eb1b0_0  \n",
      "html5lib                  1.1                pyhd3eb1b0_0  \n",
      "icc_rt                    2019.0.0             h0cc432a_1  \n",
      "icu                       58.2                 ha925a31_3  \n",
      "idna                      3.2                pyhd3eb1b0_0  \n",
      "imagecodecs               2021.8.26        py39ha1f97ea_0  \n",
      "imageio                   2.9.0              pyhd3eb1b0_0  \n",
      "imagesize                 1.2.0              pyhd3eb1b0_0  \n",
      "importlib-metadata        4.8.1            py39haa95532_0  \n",
      "importlib_metadata        4.8.1                hd3eb1b0_0  \n",
      "inflection                0.5.1            py39haa95532_0  \n",
      "iniconfig                 1.1.1              pyhd3eb1b0_0  \n",
      "intel-openmp              2021.4.0          haa95532_3556  \n",
      "intervaltree              3.1.0              pyhd3eb1b0_0  \n",
      "ipykernel                 6.4.1            py39haa95532_1  \n",
      "ipython                   7.29.0           py39hd4e2768_0  \n",
      "ipython_genutils          0.2.0              pyhd3eb1b0_1  \n",
      "ipywidgets                7.6.5              pyhd3eb1b0_1  \n",
      "isort                     5.9.3              pyhd3eb1b0_0  \n",
      "itsdangerous              2.0.1              pyhd3eb1b0_0  \n",
      "jdcal                     1.4.1              pyhd3eb1b0_0  \n",
      "jedi                      0.18.0           py39haa95532_1  \n",
      "jinja2                    2.11.3             pyhd3eb1b0_0  \n",
      "jinja2-time               0.2.0              pyhd3eb1b0_2  \n",
      "joblib                    1.1.0              pyhd3eb1b0_0  \n",
      "jpeg                      9d                   h2bbff1b_0  \n",
      "json5                     0.9.6              pyhd3eb1b0_0  \n",
      "jsonschema                3.2.0              pyhd3eb1b0_2  \n",
      "jupyter                   1.0.0            py39haa95532_7  \n",
      "jupyter_client            6.1.12             pyhd3eb1b0_0  \n",
      "jupyter_console           6.4.0              pyhd3eb1b0_0  \n",
      "jupyter_core              4.8.1            py39haa95532_0  \n",
      "jupyter_server            1.4.1            py39haa95532_0  \n",
      "jupyterlab                3.2.1              pyhd3eb1b0_1  \n",
      "jupyterlab_pygments       0.1.2                      py_0  \n",
      "jupyterlab_server         2.8.2              pyhd3eb1b0_0  \n",
      "jupyterlab_widgets        1.0.0              pyhd3eb1b0_1  \n",
      "keyring                   23.1.0           py39haa95532_0  \n",
      "kiwisolver                1.3.1            py39hd77b12b_0  \n",
      "krb5                      1.19.2               h5b6d351_0  \n",
      "lazy-object-proxy         1.6.0            py39h2bbff1b_0  \n",
      "lcms2                     2.12                 h83e58a3_0  \n",
      "lerc                      3.0                  hd77b12b_0  \n",
      "libaec                    1.0.4                h33f27b4_1  \n",
      "libarchive                3.4.2                h5e25573_0  \n",
      "libcurl                   7.78.0               h86230a5_0  \n",
      "libdeflate                1.8                  h2bbff1b_5  \n",
      "libiconv                  1.15                 h1df5818_7  \n",
      "liblief                   0.10.1               hd77b12b_1  \n",
      "libpng                    1.6.37               h2a8f88b_0  \n",
      "libspatialindex           1.9.3                h6c2663c_0  \n",
      "libssh2                   1.9.0                h7a1dbc1_1  \n",
      "libtiff                   4.2.0                hd0e1b90_0  \n",
      "libwebp                   1.2.0                h2bbff1b_0  \n",
      "libxml2                   2.9.12               h0ad7f3c_0  \n",
      "libxslt                   1.1.34               he774522_0  \n",
      "libzopfli                 1.0.3                ha925a31_0  \n",
      "llvmlite                  0.37.0           py39h23ce68f_1  \n",
      "locket                    0.2.1            py39haa95532_1  \n",
      "lxml                      4.6.3            py39h9b66d53_0  \n",
      "lz4-c                     1.9.3                h2bbff1b_1  \n",
      "lzo                       2.10                 he774522_2  \n",
      "m2w64-gcc-libgfortran     5.3.0                         6  \n",
      "m2w64-gcc-libs            5.3.0                         7  \n",
      "m2w64-gcc-libs-core       5.3.0                         7  \n",
      "m2w64-gmp                 6.1.0                         2  \n",
      "m2w64-libwinpthread-git   5.0.0.4634.697f757               2  \n",
      "markupsafe                1.1.1            py39h2bbff1b_0  \n",
      "matplotlib                3.4.3            py39haa95532_0  \n",
      "matplotlib-base           3.4.3            py39h49ac443_0  \n",
      "matplotlib-inline         0.1.2              pyhd3eb1b0_2  \n",
      "mccabe                    0.6.1            py39haa95532_1  \n",
      "menuinst                  1.4.18           py39h59b6b97_0  \n",
      "mistune                   0.8.4           py39h2bbff1b_1000  \n",
      "mkl                       2021.4.0           haa95532_640  \n",
      "mkl-service               2.4.0            py39h2bbff1b_0  \n",
      "mkl_fft                   1.3.1            py39h277e83a_0  \n",
      "mkl_random                1.2.2            py39hf11a4ad_0  \n",
      "mock                      4.0.3              pyhd3eb1b0_0  \n",
      "more-itertools            8.10.0             pyhd3eb1b0_0  \n",
      "mpmath                    1.2.1            py39haa95532_0  \n",
      "msgpack-python            1.0.2            py39h59b6b97_1  \n",
      "msys2-conda-epoch         20160418                      1  \n",
      "multipledispatch          0.6.0            py39haa95532_0  \n",
      "munkres                   1.1.4                      py_0  \n",
      "mypy_extensions           0.4.3            py39haa95532_0  \n",
      "mysql-connector-python    8.0.29                   pypi_0    pypi\n",
      "navigator-updater         0.2.1            py39haa95532_0  \n",
      "nbclassic                 0.2.6              pyhd3eb1b0_0  \n",
      "nbclient                  0.5.3              pyhd3eb1b0_0  \n",
      "nbconvert                 6.1.0            py39haa95532_0  \n",
      "nbformat                  5.1.3              pyhd3eb1b0_0  \n",
      "nest-asyncio              1.5.1              pyhd3eb1b0_0  \n",
      "networkx                  2.6.3              pyhd3eb1b0_0  \n",
      "nltk                      3.6.5              pyhd3eb1b0_0  \n",
      "nose                      1.3.7           pyhd3eb1b0_1006  \n",
      "notebook                  6.4.5            py39haa95532_0  \n",
      "numba                     0.54.1           py39hf11a4ad_0  \n",
      "numexpr                   2.7.3            py39hb80d3ca_1  \n",
      "numpy                     1.20.3           py39ha4e8547_0  \n",
      "numpy-base                1.20.3           py39hc2deb75_0  \n",
      "numpy-financial           1.0.0                    pypi_0    pypi\n",
      "numpydoc                  1.1.0              pyhd3eb1b0_1  \n",
      "olefile                   0.46               pyhd3eb1b0_0  \n",
      "openjpeg                  2.4.0                h4fc8c34_0  \n",
      "openpyxl                  3.0.9              pyhd3eb1b0_0  \n",
      "openssl                   1.1.1l               h2bbff1b_0  \n",
      "packaging                 21.0               pyhd3eb1b0_0  \n",
      "pandas                    1.3.4            py39h6214cd6_0  \n",
      "pandocfilters             1.4.3            py39haa95532_1  \n",
      "paramiko                  2.7.2                      py_0  \n",
      "parso                     0.8.2              pyhd3eb1b0_0  \n",
      "partd                     1.2.0              pyhd3eb1b0_0  \n",
      "path                      16.0.0           py39haa95532_0  \n",
      "path.py                   12.5.0               hd3eb1b0_0  \n",
      "pathlib2                  2.3.6            py39haa95532_2  \n",
      "pathspec                  0.7.0                      py_0  \n",
      "patsy                     0.5.2            py39haa95532_0  \n",
      "pep8                      1.7.1            py39haa95532_0  \n",
      "pexpect                   4.8.0              pyhd3eb1b0_3  \n",
      "pickleshare               0.7.5           pyhd3eb1b0_1003  \n",
      "pillow                    8.4.0            py39hd45dc43_0  \n",
      "pip                       21.2.4           py39haa95532_0  \n",
      "pkginfo                   1.7.1            py39haa95532_0  \n",
      "pluggy                    0.13.1           py39haa95532_0  \n",
      "ply                       3.11             py39haa95532_0  \n",
      "powershell_shortcut       0.0.1                         3  \n",
      "poyo                      0.5.0              pyhd3eb1b0_0  \n",
      "prometheus_client         0.11.0             pyhd3eb1b0_0  \n",
      "prompt-toolkit            3.0.20             pyhd3eb1b0_0  \n",
      "prompt_toolkit            3.0.20               hd3eb1b0_0  \n",
      "psutil                    5.8.0            py39h2bbff1b_1  \n",
      "ptyprocess                0.7.0              pyhd3eb1b0_2  \n",
      "py                        1.10.0             pyhd3eb1b0_0  \n",
      "py-lief                   0.10.1           py39hd77b12b_1  \n",
      "pycodestyle               2.7.0              pyhd3eb1b0_0  \n",
      "pycosat                   0.6.3            py39h2bbff1b_0  \n",
      "pycparser                 2.20                       py_2  \n",
      "pycurl                    7.44.1           py39hcd4344a_1  \n",
      "pydocstyle                6.1.1              pyhd3eb1b0_0  \n",
      "pyerfa                    2.0.0            py39h2bbff1b_0  \n",
      "pyflakes                  2.3.1              pyhd3eb1b0_0  \n",
      "pygments                  2.10.0             pyhd3eb1b0_0  \n",
      "pyjwt                     2.1.0            py39haa95532_0  \n",
      "pylint                    2.9.6            py39haa95532_1  \n",
      "pyls-spyder               0.4.0              pyhd3eb1b0_0  \n",
      "pynacl                    1.4.0            py39hbd8134f_1  \n",
      "pyodbc                    4.0.31           py39hd77b12b_0  \n",
      "pyopenssl                 21.0.0             pyhd3eb1b0_1  \n",
      "pyparsing                 3.0.4              pyhd3eb1b0_0  \n",
      "pyqt                      5.9.2            py39hd77b12b_6  \n",
      "pyreadline                2.1              py39haa95532_1  \n",
      "pyrsistent                0.18.0           py39h196d8e1_0  \n",
      "pysocks                   1.7.1            py39haa95532_0  \n",
      "pytables                  3.6.1            py39h56d22b6_1  \n",
      "pytest                    6.2.4            py39haa95532_2  \n",
      "python                    3.9.7                h6244533_1  \n",
      "python-dateutil           2.8.2              pyhd3eb1b0_0  \n",
      "python-libarchive-c       2.9                pyhd3eb1b0_1  \n",
      "python-lsp-black          1.0.0              pyhd3eb1b0_0  \n",
      "python-lsp-jsonrpc        1.0.0              pyhd3eb1b0_0  \n",
      "python-lsp-server         1.2.4              pyhd3eb1b0_0  \n",
      "python-slugify            5.0.2              pyhd3eb1b0_0  \n",
      "pytz                      2021.3             pyhd3eb1b0_0  \n",
      "pywavelets                1.1.1            py39h080aedc_4  \n",
      "pywin32                   228              py39he774522_0  \n",
      "pywin32-ctypes            0.2.0           py39haa95532_1000  \n",
      "pywinpty                  0.5.7            py39haa95532_0  \n",
      "pyyaml                    6.0              py39h2bbff1b_1  \n",
      "pyzmq                     22.2.1           py39hd77b12b_1  \n",
      "qdarkstyle                3.0.2              pyhd3eb1b0_0  \n",
      "qstylizer                 0.1.10             pyhd3eb1b0_0  \n",
      "qt                        5.9.7            vc14h73c81de_0  \n",
      "qtawesome                 1.0.2              pyhd3eb1b0_0  \n",
      "qtconsole                 5.1.1              pyhd3eb1b0_0  \n",
      "qtpy                      1.10.0             pyhd3eb1b0_0  \n",
      "regex                     2021.8.3         py39h2bbff1b_0  \n",
      "requests                  2.26.0             pyhd3eb1b0_0  \n",
      "rope                      0.19.0             pyhd3eb1b0_0  \n",
      "rtree                     0.9.7            py39h2eaa2aa_1  \n",
      "ruamel_yaml               0.15.100         py39h2bbff1b_0  \n",
      "scikit-image              0.18.3           py39hf11a4ad_0  \n",
      "scikit-learn              0.24.2           py39hf11a4ad_1  \n",
      "scikit-learn-intelex      2021.3.0         py39haa95532_0  \n",
      "scipy                     1.7.1            py39hbe87c03_2  \n",
      "seaborn                   0.11.2             pyhd3eb1b0_0  \n",
      "send2trash                1.8.0              pyhd3eb1b0_1  \n",
      "setuptools                58.0.4           py39haa95532_0  \n",
      "simplegeneric             0.8.1            py39haa95532_2  \n",
      "singledispatch            3.7.0           pyhd3eb1b0_1001  \n",
      "sip                       4.19.13          py39hd77b12b_0  \n",
      "six                       1.16.0             pyhd3eb1b0_0  \n",
      "snappy                    1.1.8                h33f27b4_0  \n",
      "sniffio                   1.2.0            py39haa95532_1  \n",
      "snowballstemmer           2.1.0              pyhd3eb1b0_0  \n",
      "sortedcollections         2.1.0              pyhd3eb1b0_0  \n",
      "sortedcontainers          2.4.0              pyhd3eb1b0_0  \n",
      "soupsieve                 2.2.1              pyhd3eb1b0_0  \n",
      "sphinx                    4.2.0              pyhd3eb1b0_1  \n",
      "sphinxcontrib             1.0              py39haa95532_1  \n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 14)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "raw_data = pd.read_csv('C:\\\\Users\\\\USER\\\\Downloads\\\\Logistic Regression Dummy Data v3 (1).txt')\n",
    "\n",
    "# print the shape\n",
    "print(raw_data.shape)\n",
    "\n",
    "print(raw_data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type of Account          0\n",
       "Account History          0\n",
       "Reason for the Loan      0\n",
       "Loan Amount              1\n",
       "Account Savings          1\n",
       "Employment History       1\n",
       "Individual Stauts        1\n",
       "Other Loans              1\n",
       "Security / Collateral    1\n",
       "Age                      1\n",
       "Residence Status         1\n",
       "Job                      1\n",
       "Completed Other loan?    1\n",
       "Good Loan                1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "\n",
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type of Account</th>\n",
       "      <th>Account History</th>\n",
       "      <th>Reason for the Loan</th>\n",
       "      <th>Loan Amount</th>\n",
       "      <th>Account Savings</th>\n",
       "      <th>Employment History</th>\n",
       "      <th>Individual Stauts</th>\n",
       "      <th>Other Loans</th>\n",
       "      <th>Security / Collateral</th>\n",
       "      <th>Age</th>\n",
       "      <th>Residence Status</th>\n",
       "      <th>Job</th>\n",
       "      <th>Completed Other loan?</th>\n",
       "      <th>Good Loan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Type C</td>\n",
       "      <td>Good</td>\n",
       "      <td>Buying a New Car</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type of Account Account History Reason for the Loan  Loan Amount  \\\n",
       "95          Type C            Good    Buying a New Car          NaN   \n",
       "\n",
       "   Account Savings Employment History Individual Stauts Other Loans  \\\n",
       "95             NaN                NaN               NaN         NaN   \n",
       "\n",
       "   Security / Collateral  Age Residence Status  Job Completed Other loan?  \\\n",
       "95                   NaN  NaN              NaN  NaN                   NaN   \n",
       "\n",
       "   Good Loan  \n",
       "95       NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the NULL observations\n",
    "\n",
    "raw_data[raw_data['Employment History'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Type of Account          0\n",
       "Account History          0\n",
       "Reason for the Loan      0\n",
       "Loan Amount              0\n",
       "Account Savings          0\n",
       "Employment History       0\n",
       "Individual Stauts        0\n",
       "Other Loans              0\n",
       "Security / Collateral    0\n",
       "Age                      0\n",
       "Residence Status         0\n",
       "Job                      0\n",
       "Completed Other loan?    0\n",
       "Good Loan                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deleting the NULL values\n",
    "raw_data = raw_data.dropna(subset = ['Employment History'])\n",
    "\n",
    "# Printing the shape\n",
    "print(raw_data.shape)\n",
    "\n",
    "# Visualize the NULL observations\n",
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of values for feature Type of Account is: 4 -- ['No Data' 'Type A' 'Type B' 'Type C']\n",
      "The number of values for feature Account History is: 3 -- ['Average (known delays)' 'Good' 'critical']\n",
      "The number of values for feature Reason for the Loan is: 9 -- ['All other' 'Buying a New Car' 'Buying a Used Car' 'Home Devices'\n",
      " 'Home furniture' 'Learning / Edu purposes' 'Renovation'\n",
      " 'Support for Business' 'TV']\n",
      "The number of values for feature Loan Amount is: 115\n",
      "The number of values for feature Account Savings is: 5 -- ['0-200' '1000+' '200-500' '500-1000' 'No Data']\n",
      "The number of values for feature Employment History is: 5 -- ['0-2 Years' '2-5 Years' '5-7 Years' '7+ Years' 'Unemployed']\n",
      "The number of values for feature Individual Stauts is: 5 -- ['Divorced' 'Female' 'Female_' 'Male' 'Married']\n",
      "The number of values for feature Other Loans is: 2 -- ['No' 'Yes']\n",
      "The number of values for feature Security / Collateral is: 4 -- ['No Security' 'Property - Real Estate' 'Savings Account' 'Vehicle']\n",
      "The number of values for feature Age is: 43\n",
      "The number of values for feature Residence Status is: 3 -- ['Free' 'House Owner' 'Renting']\n",
      "The number of values for feature Job is: 4 -- ['Not Employed' 'Professional / Management' 'Services' 'Skilled Labor']\n",
      "The number of values for feature Completed Other loan? is: 2 -- ['No' 'Yes']\n",
      "The number of values for feature Good Loan is: 2 -- ['No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Investigate all the elements whithin each Feature \n",
    "\n",
    "for column in raw_data:\n",
    "    unique_values = np.unique(raw_data[column])\n",
    "    nr_values = len(unique_values)\n",
    "    if nr_values <= 10:\n",
    "        print(\"The number of values for feature {} is: {} -- {}\".format(column, nr_values, unique_values))\n",
    "    else:\n",
    "        print(\"The number of values for feature {} is: {}\".format(column, nr_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data using seaborn Pairplots\n",
    "\n",
    "g = sns.pairplot(raw_data)\n",
    "\n",
    "# Notes: Do not run this on a big dataset. Filter the columns first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the outlier\n",
    "\n",
    "raw_data = raw_data[raw_data['Age'] < 100]\n",
    "\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data using seaborn Pairplots\n",
    "\n",
    "g = sns.pairplot(raw_data, hue = 'Good Loan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the distr of y\n",
    "\n",
    "sns.countplot(x = 'Good Loan', data = raw_data, palette = 'Set3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looping through all the features by our y variable - see if there is relationship\n",
    "\n",
    "features = ['Type of Account', 'Account History', 'Reason for the Loan',\n",
    "       'Account Savings', 'Employment History',\n",
    "       'Individual Stauts', 'Other Loans', 'Security / Collateral',\n",
    "       'Residence Status', 'Job', 'Completed Other loan?']\n",
    "\n",
    "for f in features:\n",
    "    sns.countplot(x = f, data = raw_data, palette = 'Set3', hue = 'Good Loan')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making categorical variables into numeric representation\n",
    "\n",
    "new_raw_data = pd.get_dummies(raw_data, columns = features)\n",
    "\n",
    "# Notes:\n",
    "# We can also do this with Label Encoding and OneHotEncoder from the preprocessing library\n",
    "\n",
    "print(raw_data.shape)\n",
    "# print the shape\n",
    "print(new_raw_data.shape)\n",
    "\n",
    "# Creating a new 0-1 y variable\n",
    "#new_raw_data['Loan Approved2'] = 0\n",
    "new_raw_data['Good Loan'][new_raw_data['Good Loan'] == 'Yes'] = 1\n",
    "new_raw_data['Good Loan'][new_raw_data['Good Loan'] == 'No'] = 0\n",
    "\n",
    "# Visualizing the data\n",
    "new_raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "###### We do not need to normalize / standardize the data in Logistic Regression due to the logistic function (0 or 1)\n",
    "###### Once a value crosses the decision boundary (0.5 threshold), it saturates\n",
    "###### After the 0.5 or before, there is no additional value to be added from smaller or larger values\n",
    "###### more details: https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Feature Selection\n",
    "In this example, we do not have many variables so we might use all of the data but in some cases, you have thousands of variables and you will need to filter them in order to save computational time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps of Running Feature Importance\n",
    "- Split the data into X & y\n",
    "- Run a Tree-based estimators (i.e. decision trees & random forests) \n",
    "- Run Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X & y\n",
    "\n",
    "X = new_raw_data.drop('Good Loan', axis = 1).values\n",
    "y = new_raw_data['Good Loan']\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Tree-based estimators (i.e. decision trees & random forests)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=15, criterion = 'entropy', max_depth = 10)\n",
    "dt.fit(X,y)\n",
    "\n",
    "# If you want to learn how Decesion Trees work, read here: https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "# Official Doc: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- The importance of a feature is calculated as the (normalized) total reduction of entropy (other criterions too) brought by that feature or the higher information gain\n",
    "- To understand the maths, read this: https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Feature Importance\n",
    "\n",
    "fi_col = []\n",
    "fi = []\n",
    "\n",
    "for i,column in enumerate(new_raw_data.drop('Good Loan', axis = 1)):\n",
    "    print('The feature importance for {} is : {}'.format(column, dt.feature_importances_[i]))\n",
    "    \n",
    "    fi_col.append(column)\n",
    "    fi.append(dt.feature_importances_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataframe\n",
    "fi_col\n",
    "fi\n",
    "\n",
    "fi_df = zip(fi_col, fi)\n",
    "fi_df = pd.DataFrame(fi_df, columns = ['Feature','Feature Importance'])\n",
    "fi_df\n",
    "\n",
    "\n",
    "# Ordering the data\n",
    "fi_df = fi_df.sort_values('Feature Importance', ascending = False).reset_index()\n",
    "\n",
    "# Creating columns to keep\n",
    "columns_to_keep = fi_df['Feature'][0:40]\n",
    "\n",
    "fi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- Please note that we have not normalised / scale our data\n",
    "- Please note that we have not done any feature engineering - created new features\n",
    "- Please note that we have not joined multiple datasets together\n",
    "- Please note that we have not aggregated any of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Splitting the Raw Data - Hold-out validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes\n",
    "\n",
    "print(new_raw_data.shape)\n",
    "print(new_raw_data[columns_to_keep].shape)\n",
    "\n",
    "# new_raw_data = new_raw_data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X & y\n",
    "\n",
    "X = new_raw_data[columns_to_keep].values\n",
    "X\n",
    "\n",
    "y = new_raw_data['Good Loan']\n",
    "y = y.astype(int)\n",
    "y\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out validation\n",
    "\n",
    "# first one\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size=0.2, random_state=15)\n",
    "\n",
    "# Second one\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size = 0.9, test_size=0.1, random_state=15)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_valid.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# Official Doc: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the distr of all ys\n",
    "\n",
    "ax = sns.countplot(x = y_valid, palette = \"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What is Logistic Regression \n",
    "\n",
    "- Famous statistical method for predicting two or more binary classes; not continues numbers. \n",
    "- Hence, Logistic regression is used for classification problems\n",
    "- To make it work, we transform our linear regression line into a logistic regression curve so we can get a good fit of our data (see pics below) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphical\n",
    "\n",
    "PATH = \"F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - Logistic Regression\\\\\"\n",
    "Image(filename = PATH + \"Logistic1.png\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works:\n",
    " \n",
    "- We fit an \"S\" Shape logistic Function\n",
    "- The curve tells you the porbability if a loan is good or bad\n",
    "- If we have a high Credit score, there is a high probability that it's a good loan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphical\n",
    "\n",
    "PATH = \"F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - Logistic Regression\\\\\"\n",
    "Image(filename = PATH + \"Logistic2.png\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Maximum likelihood works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphical\n",
    "\n",
    "PATH = \"F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - Logistic Regression\\\\\"\n",
    "Image(filename = PATH + \"Logistic3.png\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link for Discrete Variables: https://youtu.be/vN5cNN2-HWE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Running Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training my model\n",
    "\n",
    "log_reg = LogisticRegression(random_state=10, solver = 'lbfgs')\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# SKLearn doc: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods we can use in Logistic\n",
    "\n",
    "# predict - Predict class labels for samples in X\n",
    "log_reg.predict(X_train)\n",
    "y_pred = log_reg.predict(X_train)\n",
    "\n",
    "# predict_proba - Probability estimates\n",
    "pred_proba = log_reg.predict_proba(X_train)\n",
    "\n",
    "# coef_ - Coefficient of the features in the decision function\n",
    "log_reg.coef_\n",
    "\n",
    "# score- Returns the mean accuracy on the given test data and labels - below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on Train\n",
    "print(\"The Training Accuracy is: \", log_reg.score(X_train, y_train))\n",
    "\n",
    "# Accuracy on Test\n",
    "print(\"The Testing Accuracy is: \", log_reg.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix function\n",
    "\n",
    "def plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n",
    "    \"\"\"Plots a confusion matrix.\"\"\"\n",
    "    if classes is not None:\n",
    "        sns.heatmap(cm, cmap=\"YlGnBu\", xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., annot=True, annot_kws={'size':50})\n",
    "    else:\n",
    "        sns.heatmap(cm, vmin=0., vmax=1.)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing cm\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "plot_confusion_matrix(cm_norm, classes = log_reg.classes_, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.sum(axis=1)\n",
    "cm_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\n",
    "\n",
    "FP = cm.sum(axis=0) - np.diag(cm)\n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP / (TP + FN)\n",
    "print(\"The True Positive Rate is:\", TPR)\n",
    "\n",
    "# Precision or positive predictive value\n",
    "PPV = TP / (TP + FP)\n",
    "print(\"The Precision is:\", PPV)\n",
    "\n",
    "# False positive rate or False alarm rate\n",
    "FPR = FP / (FP + TN)\n",
    "print(\"The False positive rate is:\", FPR)\n",
    "\n",
    "\n",
    "# False negative rate or Miss Rate\n",
    "FNR = FN / (FN + TP)\n",
    "print(\"The False Negative Rate is: \", FNR)\n",
    "\n",
    "\n",
    "\n",
    "##Total averages :\n",
    "print(\"\")\n",
    "print(\"The average TPR is:\", TPR.sum()/2)\n",
    "print(\"The average Precision is:\", PPV.sum()/2)\n",
    "print(\"The average False positive rate is:\", FPR.sum()/2)\n",
    "print(\"The average False Negative Rate is:\", FNR.sum()/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logarithmic loss - or Log Loss - or cross-entropy loss\n",
    "\n",
    "- Log Loss is an error metric\n",
    "- This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier’s predictions. \n",
    "\n",
    "- Why it's important? For example, imagine having 2 models / classifiers that both predict one observation correctly (Good Loan). However, 1 classifier has a predicted probability of 0.54 and the other 0.95. Which one will you choose? Classification Accuracy will not help here as it will get both on 100%\n",
    "\n",
    "- Doc: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Log loss on training\n",
    "print(\"The Log Loss on Training is: \", log_loss(y_train, pred_proba))\n",
    "\n",
    "# Running Log loss on testing\n",
    "pred_proba_t = log_reg.predict_proba(X_test)\n",
    "print(\"The Log Loss on Testing Dataset is: \", log_loss(y_test, pred_proba_t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyper Parameter Tuning\n",
    "\n",
    "- We will loop over parameter C (Inverse of regularization strength).\n",
    "- Inverse of regularization strength helps to avoid overfitting - it penalizes large values of your parameters\n",
    "- It also helps to find Global Minimum by moving to better \"solutions\" from local minimum to global minimum\n",
    "- The values of C to search should be n-equally-spaced values in log space ranging from 1e-5 to 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.geomspace(1e-5, 1e5, num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a range for C values\n",
    "np.geomspace(1e-5, 1e5, num=20)\n",
    "\n",
    "# ploting it\n",
    "plt.plot(np.geomspace(1e-5, 1e5, num=20)) #  uniformly distributed in log space\n",
    "plt.plot(np.linspace(1e-5, 1e5, num=20)) # uniformly distributed in linear space, instead of log space\n",
    "# plt.plot(np.logspace(np.log10(1e-5) , np.log10(1e5) , num=20)) # same as geomspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping over the parameters\n",
    "\n",
    "C_List = np.geomspace(1e-5, 1e5, num=20)\n",
    "CA = []\n",
    "Logarithmic_Loss = []\n",
    "\n",
    "for c in C_List:\n",
    "    log_reg2 = LogisticRegression(random_state=10, solver = 'lbfgs', C=c)\n",
    "    log_reg2.fit(X_train, y_train)\n",
    "    score = log_reg2.score(X_test, y_test)\n",
    "    CA.append(score)\n",
    "    print(\"The CA of C parameter {} is {}:\".format(c, score))\n",
    "    pred_proba_t = log_reg2.predict_proba(X_test)\n",
    "    log_loss2 = log_loss(y_test, pred_proba_t)\n",
    "    Logarithmic_Loss.append(log_loss2)\n",
    "    print(\"The Logg Loss of C parameter {} is {}:\".format(c, log_loss2))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the outcomes in a Table\n",
    "\n",
    "# reshaping\n",
    "CA2 = np.array(CA).reshape(20,)\n",
    "Logarithmic_Loss2 = np.array(Logarithmic_Loss).reshape(20,)\n",
    "\n",
    "# zip\n",
    "outcomes = zip(C_List, CA2, Logarithmic_Loss2)\n",
    "\n",
    "#df\n",
    "df_outcomes = pd.DataFrame(outcomes, columns = [\"C_List\", 'CA2','Logarithmic_Loss2'])\n",
    "\n",
    "#print\n",
    "df_outcomes\n",
    "\n",
    "# Ordering the data (sort_values)\n",
    "df_outcomes.sort_values(\"Logarithmic_Loss2\", ascending = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of doing the above\n",
    "# Scikit-learn offers a LogisticRegressionCV module which implements Logistic Regression \n",
    "# with builtin cross-validation to find out the optimal C parameter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, random_state=0, shuffle=True)\n",
    "\n",
    "# Logistic Reg CV\n",
    "Log_reg3 = LogisticRegressionCV(random_state=15, Cs = C_List, solver ='lbfgs')\n",
    "Log_reg3.fit(X_train, y_train)\n",
    "print(\"The CA is:\", Log_reg3.score(X_test, y_test))\n",
    "pred_proba_t = Log_reg3.predict_proba(X_test)\n",
    "log_loss3 = log_loss(y_test, pred_proba_t)\n",
    "print(\"The Logistic Loss is: \", log_loss3)\n",
    "\n",
    "print(\"The optimal C parameter is: \", Log_reg3.C_)\n",
    "\n",
    "\n",
    "\n",
    "# Doc: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation:\n",
    "Advantage: K-fold cross validation uses all the training data to train the model, by applying k different splits; repeated train-test splits converge to the true accuracy given that the training data is representable for the underlying distribution; however in practise this is often overoptimistic.\n",
    "Disadvantage: The disadvantage of this method is that the training algorithm has to be rerun from the beginning k times, which means it takes k times as much computation to get an evaluation. Additionally, if you want to test the performance on a completely new dataset that the algorithm has never seen, you cannot do this with k-fold cross validation.\n",
    "\n",
    "### Hold-out:\n",
    "Advantage: The advantage of Hold-out is that you can test how your model performs on completely unseen data that you haven't used when training the model. Additionally, Hold-out is usually much faster and less computationally expensive. \n",
    "Disadvantage: The evaluation may depend heavily on which data points end up in the training set and which end up in the test set, and thus the evaluation may be significantly different depending on how the division is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Maybe we have a different metric we want to track\n",
    "\n",
    "# Looping over the parameters\n",
    "\n",
    "C_List = np.geomspace(1e-5, 1e5, num=20)\n",
    "CA = []\n",
    "Logarithmic_Loss = []\n",
    "\n",
    "for c in C_List:\n",
    "    log_reg2 = LogisticRegression(random_state=10, solver = 'lbfgs', C=c)\n",
    "    log_reg2.fit(X_train, y_train)\n",
    "    score = log_reg2.score(X_test, y_test)\n",
    "    CA.append(score)\n",
    "    print(\"The CA of C parameter {} is {}:\".format(c, score))\n",
    "    pred_proba_t = log_reg2.predict_proba(X_test)\n",
    "    log_loss2 = log_loss(y_test, pred_proba_t)\n",
    "    Logarithmic_Loss.append(log_loss2)\n",
    "    print(\"The Logg Loss of C parameter {} is {}:\".format(c, log_loss2))\n",
    "    print(\"\")\n",
    "    \n",
    "    y_pred = log_reg2.predict(X_train)\n",
    "    cm = confusion_matrix(y_train, y_pred)\n",
    "    cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "    plot_confusion_matrix(cm_norm, classes = log_reg.classes_, title='Confusion matrix')\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Dummy Classifier\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "score = dummy_clf.score(X_test, y_test)\n",
    "\n",
    "pred_proba_t = dummy_clf.predict_proba(X_test)\n",
    "log_loss2 = log_loss(y_test, pred_proba_t)\n",
    "\n",
    "print(\"Testing Acc:\", score)\n",
    "print(\"Log Loss:\", log_loss2)\n",
    "\n",
    "\n",
    "# Doc: https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Model with Selected Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model \n",
    "\n",
    "log_reg3 = LogisticRegression(random_state=10, solver = 'lbfgs', C=784.759970)\n",
    "log_reg3.fit(X_train, y_train)\n",
    "score = log_reg3.score(X_valid, y_valid)\n",
    "\n",
    "pred_proba_t = log_reg3.predict_proba(X_valid)\n",
    "log_loss2 = log_loss(y_valid, pred_proba_t)\n",
    "\n",
    "print(\"Testing Acc:\", score)\n",
    "print(\"Log Loss:\", log_loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. How to use our L. Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "\n",
    "- Option 1: Deploy the model in a CRM System or the Cloud or Viz tools and automaticaly decide if a customer should get his loan approved or not\n",
    "\n",
    "\n",
    "- Option 2: Analyse the factors that affect a good/bad loan and help the business understand this. Then the business can educate their clients what they need to improve in order to get a loan\n",
    "\n",
    "\n",
    "- Option 3: Deploy this model in an open bank website where customers can automatically see if their loan will get approved or not; saving time & cost for the bank/business\n",
    "\n",
    "\n",
    "- Option 4 - What do you think?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
